---
title: "Monitoring & Observability"
description: "Complete monitoring stack voor 3WM met Prometheus, Grafana en Langfuse"
icon: "chart-line"
---

# Monitoring & Observability

Een robuuste monitoring setup is essentieel voor het succesvol draaien van 3WM in productie. Deze guide beschrijft de complete observability stack.

## Monitoring Stack Overview

<CardGroup cols={4}>
  <Card title="Prometheus" icon="chart-simple">
    **Metrics Collection**
    - Time-series database
    - Pull-based metrics
    - Alert rules
    - Service discovery
  </Card>
  
  <Card title="Grafana" icon="chart-line">
    **Visualization**
    - Real-time dashboards
    - Custom panels
    - Alert notifications
    - Data exploration
  </Card>
  
  <Card title="Langfuse" icon="brain">
    **LLM Observability**
    - Prompt tracking
    - Token usage
    - Cost monitoring
    - Performance analysis
  </Card>
  
  <Card title="Loki" icon="scroll">
    **Log Aggregation**
    - Centralized logging
    - Log correlation
    - Full-text search
    - Log-based alerts
  </Card>
</CardGroup>

## Quick Start

### 1. Deploy Monitoring Stack

```bash
# Clone monitoring configs
git clone https://github.com/sky-dust-intelligence/3wm-monitoring.git
cd 3wm-monitoring

# Start all monitoring services
docker-compose up -d

# Verify services
docker-compose ps
```

### 2. Access Dashboards

<Tabs>
  <Tab title="Grafana">
    ```
    URL: http://localhost:3000
    Username: admin
    Password: admin (change immediately)
    ```
  </Tab>
  
  <Tab title="Prometheus">
    ```
    URL: http://localhost:9090
    No authentication by default
    ```
  </Tab>
  
  <Tab title="Langfuse">
    ```
    URL: http://localhost:3001
    Create account on first visit
    ```
  </Tab>
</Tabs>

## Metrics Configuration

### Application Metrics

```python
# app/core/metrics.py
from prometheus_client import Counter, Histogram, Gauge, Info
import time

# Request metrics
http_requests_total = Counter(
    'http_requests_total',
    'Total HTTP requests',
    ['method', 'endpoint', 'status']
)

request_duration_seconds = Histogram(
    'request_duration_seconds',
    'HTTP request duration',
    ['method', 'endpoint']
)

# Business metrics
invoices_processed_total = Counter(
    'invoices_processed_total',
    'Total invoices processed',
    ['status', 'source']
)

invoice_processing_duration = Histogram(
    'invoice_processing_duration_seconds',
    'Invoice processing time',
    ['extraction_method', 'status']
)

matching_accuracy = Gauge(
    'matching_accuracy_percentage',
    'Current 3-way matching accuracy',
    ['vendor_category']
)

# LLM metrics
llm_tokens_used = Counter(
    'llm_tokens_used_total',
    'Total LLM tokens consumed',
    ['model', 'operation']
)

llm_cost_dollars = Counter(
    'llm_cost_dollars_total',
    'Total LLM API costs',
    ['model', 'operation']
)

# System info
app_info = Info('app_info', 'Application information')
app_info.info({
    'version': '2.0.0',
    'environment': 'production',
    'python_version': '3.13'
})
```

### Middleware Integration

```python
# app/middleware/metrics.py
from fastapi import Request
import time

async def metrics_middleware(request: Request, call_next):
    """Track request metrics."""
    start_time = time.time()
    
    # Process request
    response = await call_next(request)
    
    # Record metrics
    duration = time.time() - start_time
    
    http_requests_total.labels(
        method=request.method,
        endpoint=request.url.path,
        status=response.status_code
    ).inc()
    
    request_duration_seconds.labels(
        method=request.method,
        endpoint=request.url.path
    ).observe(duration)
    
    return response
```

## Prometheus Configuration

### prometheus.yml

```yaml
global:
  scrape_interval: 15s
  evaluation_interval: 15s
  external_labels:
    cluster: '3wm-prod'
    region: 'eu-west-1'

# Alertmanager configuration
alerting:
  alertmanagers:
    - static_configs:
        - targets: ['alertmanager:9093']

# Load rules
rule_files:
  - '/etc/prometheus/rules/*.yml'

# Scrape configurations
scrape_configs:
  # 3WM API metrics
  - job_name: '3wm-api'
    static_configs:
      - targets: ['api:8000']
    metrics_path: '/metrics'
    scrape_interval: 10s

  # PostgreSQL metrics
  - job_name: 'postgres'
    static_configs:
      - targets: ['postgres-exporter:9187']
    params:
      target: ['postgres:5432']

  # Redis metrics
  - job_name: 'redis'
    static_configs:
      - targets: ['redis-exporter:9121']

  # Node metrics
  - job_name: 'node'
    static_configs:
      - targets: ['node-exporter:9100']

  # Cadvisor for container metrics
  - job_name: 'cadvisor'
    static_configs:
      - targets: ['cadvisor:8080']
```

### Alert Rules

```yaml
# rules/alerts.yml
groups:
  - name: 3wm_alerts
    interval: 30s
    rules:
      # API Health
      - alert: APIDown
        expr: up{job="3wm-api"} == 0
        for: 1m
        labels:
          severity: critical
          team: backend
        annotations:
          summary: "3WM API is down"
          description: "API has been down for more than 1 minute"

      # High Error Rate
      - alert: HighErrorRate
        expr: |
          rate(http_requests_total{status=~"5.."}[5m]) 
          / rate(http_requests_total[5m]) > 0.05
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High error rate detected"
          description: "Error rate is above 5% (current: {{ $value | humanizePercentage }})"

      # Slow Response Time
      - alert: SlowResponseTime
        expr: |
          histogram_quantile(0.95, 
            rate(request_duration_seconds_bucket[5m])
          ) > 2
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "API response time is slow"
          description: "95th percentile response time is above 2s"

      # Invoice Processing Issues
      - alert: InvoiceProcessingBacklog
        expr: |
          rate(invoices_processed_total{status="failed"}[15m]) 
          / rate(invoices_processed_total[15m]) > 0.1
        for: 15m
        labels:
          severity: warning
          team: ml
        annotations:
          summary: "High invoice processing failure rate"
          description: "More than 10% of invoices are failing"

      # LLM Cost Alert
      - alert: HighLLMCost
        expr: |
          increase(llm_cost_dollars_total[1h]) > 100
        labels:
          severity: warning
          team: finance
        annotations:
          summary: "High LLM API costs"
          description: "LLM costs exceeded $100 in the last hour"

      # Database Issues
      - alert: DatabaseConnectionPoolExhausted
        expr: |
          pg_stat_database_numbackends 
          / pg_settings_max_connections > 0.8
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Database connection pool nearly exhausted"
          description: "Using {{ $value | humanizePercentage }} of available connections"

      # Disk Space
      - alert: LowDiskSpace
        expr: |
          node_filesystem_avail_bytes{mountpoint="/"} 
          / node_filesystem_size_bytes{mountpoint="/"} < 0.1
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Low disk space"
          description: "Less than 10% disk space remaining"
```

## Grafana Dashboards

### 3WM Overview Dashboard

```json
{
  "dashboard": {
    "title": "3WM System Overview",
    "panels": [
      {
        "title": "Request Rate",
        "gridPos": {"x": 0, "y": 0, "w": 6, "h": 8},
        "targets": [{
          "expr": "sum(rate(http_requests_total[5m])) by (status)"
        }]
      },
      {
        "title": "Response Time (p95)",
        "gridPos": {"x": 6, "y": 0, "w": 6, "h": 8},
        "targets": [{
          "expr": "histogram_quantile(0.95, sum(rate(request_duration_seconds_bucket[5m])) by (le))"
        }]
      },
      {
        "title": "Invoice Processing Rate",
        "gridPos": {"x": 12, "y": 0, "w": 6, "h": 8},
        "targets": [{
          "expr": "sum(rate(invoices_processed_total[5m])) by (status)"
        }]
      },
      {
        "title": "3-Way Match Success Rate",
        "gridPos": {"x": 18, "y": 0, "w": 6, "h": 8},
        "targets": [{
          "expr": "avg(matching_accuracy_percentage)"
        }]
      }
    ]
  }
}
```

### LLM Monitoring Dashboard

```json
{
  "dashboard": {
    "title": "LLM Usage & Costs",
    "panels": [
      {
        "title": "Token Usage by Model",
        "targets": [{
          "expr": "sum(rate(llm_tokens_used_total[5m])) by (model)"
        }]
      },
      {
        "title": "API Costs (Hourly)",
        "targets": [{
          "expr": "sum(increase(llm_cost_dollars_total[1h])) by (model)"
        }]
      },
      {
        "title": "Average Tokens per Request",
        "targets": [{
          "expr": "rate(llm_tokens_used_total[5m]) / rate(invoices_processed_total[5m])"
        }]
      }
    ]
  }
}
```

## Langfuse Integration

### Setup

```python
# app/core/langfuse_config.py
from langfuse import Langfuse
from app.core.config import settings

# Initialize Langfuse client
langfuse = Langfuse(
    public_key=settings.LANGFUSE_PUBLIC_KEY,
    secret_key=settings.LANGFUSE_SECRET_KEY,
    host=settings.LANGFUSE_HOST  # Self-hosted instance
)
```

### Trace LLM Calls

```python
# app/agents/base.py
from langfuse.decorators import observe, langfuse_context

class BaseAgent:
    @observe()
    async def process_document(self, document: Document) -> Result:
        """Process document with full observability."""
        
        # Start trace
        trace = langfuse_context.get_current_trace()
        
        # Add metadata
        trace.update_metadata({
            "document_id": document.id,
            "document_type": document.type,
            "source": document.source
        })
        
        # Track each step
        with trace.span("extraction") as span:
            extracted = await self.extract(document)
            span.update({
                "model": "gpt-4",
                "tokens": extracted.token_count,
                "confidence": extracted.confidence
            })
        
        with trace.span("validation") as span:
            validated = await self.validate(extracted)
            span.update({
                "errors": len(validated.errors),
                "warnings": len(validated.warnings)
            })
        
        # Score the trace
        trace.score(
            name="accuracy",
            value=validated.accuracy,
            comment="Extraction accuracy"
        )
        
        return Result(extracted, validated)
```

### Custom Metrics

```python
# Track custom business metrics in Langfuse
@observe()
async def process_invoice_batch(invoices: List[Invoice]):
    """Process batch with detailed tracking."""
    
    results = []
    total_cost = 0
    
    for invoice in invoices:
        result = await process_invoice(invoice)
        results.append(result)
        
        # Track per-invoice metrics
        langfuse_context.update_current_observation(
            metadata={
                "invoice_id": invoice.id,
                "vendor": invoice.vendor,
                "amount": invoice.amount,
                "processing_time": result.duration,
                "match_score": result.match_score
            },
            usage={
                "input": result.input_tokens,
                "output": result.output_tokens,
                "total": result.total_tokens,
                "unit": "tokens"
            }
        )
        
        total_cost += result.cost
    
    # Track batch metrics
    langfuse_context.update_current_observation(
        metadata={
            "batch_size": len(invoices),
            "success_rate": sum(1 for r in results if r.success) / len(results),
            "total_cost": total_cost,
            "avg_processing_time": sum(r.duration for r in results) / len(results)
        }
    )
``` 